{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Personal Financial Agent — Evaluation & Synthetic Data Generation\n",
                "\n",
                "This notebook demonstrates the full evaluation lifecycle for our Romanian Personal Financial Agent,\n",
                "directly adapted from **AIE9 Sessions 9-10**.\n",
                "\n",
                "## Structure\n",
                "1. **Synthetic Data Generation** — Generate test questions from financial documents using RAGAS\n",
                "2. **RAG Evaluation — Baseline** — Evaluate with naive top-k retrieval\n",
                "3. **RAG Evaluation — Improved** — Add Cohere reranking and compare scores\n",
                "4. **Agent Evaluation** — Test tool routing, topic adherence, MiFID II compliance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup & Imports\n",
                "import os\n",
                "import sys\n",
                "import asyncio\n",
                "import json\n",
                "import pandas as pd\n",
                "from IPython.display import display, HTML, Markdown\n",
                "\n",
                "# Add app to path\n",
                "sys.path.insert(0, '/app')\n",
                "\n",
                "from app.config import settings\n",
                "from app.services.rag_service import rag_service\n",
                "\n",
                "print(f'OpenAI API Key: {settings.openai_api_key[:8]}...')\n",
                "print(f'Qdrant: {settings.qdrant_host}:{settings.qdrant_port}')\n",
                "print(f'Collection: {settings.qdrant_collection}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Synthetic Data Generation (SDG)\n",
                "\n",
                "Using RAGAS `TestsetGenerator` to create synthetic question-answer pairs from our Romanian\n",
                "financial documents. This follows the AIE9 Session 9 pattern.\n",
                "\n",
                "The generator creates three types of questions:\n",
                "- **Simple** — Single-fact retrieval questions\n",
                "- **Multi-Context** — Questions requiring information from multiple chunks\n",
                "- **Reasoning** — Questions requiring inference from retrieved information"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load documents for SDG\n",
                "from langchain_community.document_loaders import PyMuPDFLoader\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "\n",
                "# Load all PDFs from documents folder\n",
                "import glob\n",
                "\n",
                "pdf_files = glob.glob('/app/documents/*.pdf')\n",
                "documents = []\n",
                "for pdf in pdf_files:\n",
                "    loader = PyMuPDFLoader(pdf)\n",
                "    documents.extend(loader.load())\n",
                "\n",
                "print(f'Loaded {len(documents)} pages from {len(pdf_files)} PDF files')\n",
                "for doc in documents[:3]:\n",
                "    print(f'  - {doc.metadata.get(\"source\", \"unknown\")}: {doc.page_content[:100]}...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic test set\n",
                "from ragas.testset import TestsetGenerator\n",
                "from ragas.llms import LangchainLLMWrapper\n",
                "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
                "\n",
                "# Setup LLM and embeddings for SDG\n",
                "generator_llm = LangchainLLMWrapper(ChatOpenAI(model='gpt-4o', api_key=settings.openai_api_key))\n",
                "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(\n",
                "    model=settings.embedding_model,\n",
                "    api_key=settings.openai_api_key\n",
                "))\n",
                "\n",
                "# Create test set generator\n",
                "generator = TestsetGenerator(\n",
                "    llm=generator_llm,\n",
                "    embedding_model=generator_embeddings,\n",
                ")\n",
                "\n",
                "# Generate synthetic test set\n",
                "testset = generator.generate_with_langchain_docs(\n",
                "    documents=documents,\n",
                "    testset_size=10,\n",
                ")\n",
                "\n",
                "test_df = testset.to_pandas()\n",
                "print(f'Generated {len(test_df)} synthetic test questions')\n",
                "display(test_df[['question', 'ground_truth']].head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If SDG fails (e.g., not enough documents), use manually curated test questions\n",
                "# This is a fallback that ensures the evaluation can always run\n",
                "\n",
                "MANUAL_TEST_QUESTIONS = [\n",
                "    {\n",
                "        'question': 'Ce sunt titlurile de stat TEZAUR?',\n",
                "        'ground_truth': 'Titlurile TEZAUR sunt instrumente financiare emise de Ministerul Finantelor din Romania, destinate exclusiv persoanelor fizice rezidente. Au maturitati de 1, 3 sau 5 ani, dobanda fixa, si sunt 100% garantate de statul roman. Sunt scutite de impozit pe venit.',\n",
                "    },\n",
                "    {\n",
                "        'question': 'Care sunt diferentele intre TEZAUR si FIDELIS?',\n",
                "        'ground_truth': 'TEZAUR nu se tranzactioneaza pe bursa si este scutit de impozit. FIDELIS este listat la BVB, poate fi tranzactionat pe piata secundara, si este impozitat cu 10% din 2023.',\n",
                "    },\n",
                "    {\n",
                "        'question': 'Ce avantaje are TEZAUR fata de depozitele bancare?',\n",
                "        'ground_truth': 'Nu exista risc de pierdere a capitalului investit. Dobanzile sunt mai mari decat la depozitele bancare. Scutire de impozit pe venit. Accesibile de la 1 RON.',\n",
                "    },\n",
                "    {\n",
                "        'question': 'Cum se pot achizitiona titlurile FIDELIS?',\n",
                "        'ground_truth': 'FIDELIS sunt listate la BVB si pot fi cumparate sau vandute pe piata secundara. Dobanda fixa, platita semestrial sub forma de cupon.',\n",
                "    },\n",
                "    {\n",
                "        'question': 'Ce maturitati au titlurile de stat romanesti?',\n",
                "        'ground_truth': 'Titlurile TEZAUR si FIDELIS au maturitati de 1 an, 3 ani sau 5 ani. FIDELIS poate fi denominat in LEI sau EURO.',\n",
                "    },\n",
                "]\n",
                "\n",
                "# Use SDG results if available, otherwise fall back to manual\n",
                "try:\n",
                "    if len(test_df) >= 5:\n",
                "        eval_questions = test_df['question'].tolist()\n",
                "        eval_ground_truths = test_df['ground_truth'].tolist()\n",
                "        print(f'Using {len(eval_questions)} SDG-generated questions')\n",
                "    else:\n",
                "        raise ValueError('Not enough SDG questions')\n",
                "except:\n",
                "    eval_questions = [q['question'] for q in MANUAL_TEST_QUESTIONS]\n",
                "    eval_ground_truths = [q['ground_truth'] for q in MANUAL_TEST_QUESTIONS]\n",
                "    print(f'Using {len(eval_questions)} manually curated questions')\n",
                "\n",
                "for i, q in enumerate(eval_questions, 1):\n",
                "    print(f'{i}. {q}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. RAG Evaluation — Baseline (No Reranking)\n",
                "\n",
                "First, we evaluate the RAG pipeline with naive top-5 similarity search — **no reranking**.\n",
                "This establishes our baseline scores that we'll improve upon."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run baseline RAG evaluation (no reranking)\n",
                "from datasets import Dataset\n",
                "\n",
                "async def evaluate_rag(questions, ground_truths, use_reranking=False):\n",
                "    \"\"\"Run RAG pipeline and collect results for RAGAS evaluation.\"\"\"\n",
                "    answers = []\n",
                "    contexts = []\n",
                "    \n",
                "    llm = ChatOpenAI(model=settings.specialist_model, api_key=settings.openai_api_key)\n",
                "    \n",
                "    for question in questions:\n",
                "        # Retrieve documents\n",
                "        docs = await rag_service.query(question, use_reranking=use_reranking)\n",
                "        context_texts = [doc.page_content for doc in docs]\n",
                "        \n",
                "        # Generate answer\n",
                "        context_str = '\\n\\n'.join(context_texts)\n",
                "        prompt = f'Based on the following context, answer the question.\\n\\nContext:\\n{context_str}\\n\\nQuestion: {question}\\n\\nAnswer:'\n",
                "        response = await llm.ainvoke(prompt)\n",
                "        \n",
                "        answers.append(response.content)\n",
                "        contexts.append(context_texts)\n",
                "    \n",
                "    return answers, contexts\n",
                "\n",
                "# Run baseline\n",
                "print('Running baseline RAG evaluation (no reranking)...')\n",
                "baseline_answers, baseline_contexts = await evaluate_rag(\n",
                "    eval_questions, eval_ground_truths, use_reranking=False\n",
                ")\n",
                "print(f'Generated {len(baseline_answers)} answers')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ragas import evaluate as ragas_evaluate\n",
                "from ragas.metrics import (\n",
                "    faithfulness,\n",
                "    answer_relevancy,\n",
                "    context_precision,\n",
                "    context_recall,\n",
                ")\n",
                "\n",
                "# Create RAGAS dataset\n",
                "baseline_dataset = Dataset.from_dict({\n",
                "    'question': eval_questions,\n",
                "    'answer': baseline_answers,\n",
                "    'contexts': baseline_contexts,\n",
                "    'ground_truth': eval_ground_truths,\n",
                "})\n",
                "\n",
                "# Run RAGAS evaluation\n",
                "print('Running RAGAS metrics on baseline...')\n",
                "baseline_result = ragas_evaluate(\n",
                "    dataset=baseline_dataset,\n",
                "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
                ")\n",
                "\n",
                "baseline_scores = {k: round(v, 4) for k, v in baseline_result.items() if isinstance(v, (int, float))}\n",
                "print('\\n=== Baseline RAG Scores ===')\n",
                "for metric, score in baseline_scores.items():\n",
                "    bar = '█' * int(score * 20) + '░' * (20 - int(score * 20))\n",
                "    print(f'  {metric:<25} {bar} {score:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. RAG Evaluation — Improved (With Cohere Reranking)\n",
                "\n",
                "Now we add **Cohere Rerank** (`rerank-multilingual-v3.0`) to the pipeline.\n",
                "This retrieves top-5 candidates and reranks them down to top-3,\n",
                "improving precision and relevance.\n",
                "\n",
                "This is the **iteration story** required for certification — we show measurable improvement."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run improved RAG evaluation (with Cohere reranking)\n",
                "print('Running improved RAG evaluation (with Cohere reranking)...')\n",
                "reranked_answers, reranked_contexts = await evaluate_rag(\n",
                "    eval_questions, eval_ground_truths, use_reranking=True\n",
                ")\n",
                "\n",
                "# Create RAGAS dataset\n",
                "reranked_dataset = Dataset.from_dict({\n",
                "    'question': eval_questions,\n",
                "    'answer': reranked_answers,\n",
                "    'contexts': reranked_contexts,\n",
                "    'ground_truth': eval_ground_truths,\n",
                "})\n",
                "\n",
                "# Run RAGAS evaluation\n",
                "print('Running RAGAS metrics on reranked pipeline...')\n",
                "reranked_result = ragas_evaluate(\n",
                "    dataset=reranked_dataset,\n",
                "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
                ")\n",
                "\n",
                "reranked_scores = {k: round(v, 4) for k, v in reranked_result.items() if isinstance(v, (int, float))}\n",
                "print('\\n=== Reranked RAG Scores ===')\n",
                "for metric, score in reranked_scores.items():\n",
                "    bar = '█' * int(score * 20) + '░' * (20 - int(score * 20))\n",
                "    print(f'  {metric:<25} {bar} {score:.4f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Side-by-side comparison\n",
                "print('\\n' + '='*70)\n",
                "print('COMPARISON: Baseline vs Reranked (Cohere rerank-multilingual-v3.0)')\n",
                "print('='*70)\n",
                "print(f'{\"Metric\":<25} {\"Baseline\":>10} {\"Reranked\":>10} {\"Delta\":>10} {\"Improved?\":>10}')\n",
                "print('-'*70)\n",
                "\n",
                "comparison_data = []\n",
                "for metric in baseline_scores:\n",
                "    b = baseline_scores.get(metric, 0)\n",
                "    r = reranked_scores.get(metric, 0)\n",
                "    delta = r - b\n",
                "    improved = '✅' if delta > 0 else ('⚠️' if delta == 0 else '❌')\n",
                "    delta_str = f'+{delta:.4f}' if delta >= 0 else f'{delta:.4f}'\n",
                "    print(f'{metric:<25} {b:>10.4f} {r:>10.4f} {delta_str:>10} {improved:>10}')\n",
                "    comparison_data.append({\n",
                "        'Metric': metric,\n",
                "        'Baseline': b,\n",
                "        'Reranked': r,\n",
                "        'Delta': delta,\n",
                "        'Improved': improved,\n",
                "    })\n",
                "\n",
                "print('\\n')\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "display(comparison_df.style.format({'Baseline': '{:.4f}', 'Reranked': '{:.4f}', 'Delta': '{:+.4f}'}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Agent Evaluation\n",
                "\n",
                "We evaluate the full LangGraph Supervisor agent on:\n",
                "- **Tool Call Accuracy** — Does it route to the right tool?\n",
                "- **Topic Adherence** — Does the response stay on topic?\n",
                "- **MiFID II Compliance** — Does it add disclaimers when discussing investments?\n",
                "- **Language Detection** — Does it respond in the user's language?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Agent evaluation\n",
                "from app.services.agent_service import agent_service\n",
                "\n",
                "DEMO_USER_ID = '00000000-0000-0000-0000-000000000001'\n",
                "\n",
                "AGENT_TEST_SCENARIOS = [\n",
                "    {\n",
                "        'category': 'RAG Query',\n",
                "        'message': 'Ce este TEZAUR?',\n",
                "        'expected_topics': ['TEZAUR', 'titluri de stat', 'garantat'],\n",
                "        'should_have_disclaimer': True,\n",
                "    },\n",
                "    {\n",
                "        'category': 'Market Search',\n",
                "        'message': 'Care este cursul EUR/RON astazi?',\n",
                "        'expected_topics': ['EUR', 'RON', 'curs'],\n",
                "        'should_have_disclaimer': False,\n",
                "    },\n",
                "    {\n",
                "        'category': 'Goals Query',\n",
                "        'message': 'Care sunt obiectivele mele financiare?',\n",
                "        'expected_topics': ['obiectiv', 'RON'],\n",
                "        'should_have_disclaimer': False,\n",
                "    },\n",
                "    {\n",
                "        'category': 'Language (EN)',\n",
                "        'message': 'What are the differences between TEZAUR and FIDELIS?',\n",
                "        'expected_topics': ['TEZAUR', 'FIDELIS'],\n",
                "        'should_have_disclaimer': True,\n",
                "    },\n",
                "]\n",
                "\n",
                "agent_results = []\n",
                "for i, scenario in enumerate(AGENT_TEST_SCENARIOS, 1):\n",
                "    print(f'\\n--- Scenario {i}: {scenario[\"category\"]} ---')\n",
                "    print(f'Message: {scenario[\"message\"]}')\n",
                "    \n",
                "    response = await agent_service.chat(\n",
                "        message=scenario['message'],\n",
                "        user_id=DEMO_USER_ID,\n",
                "        session_id=f'eval-notebook-{i}',\n",
                "    )\n",
                "    \n",
                "    # Score\n",
                "    topic_hits = sum(1 for t in scenario['expected_topics'] if t.lower() in response.lower())\n",
                "    topic_score = topic_hits / len(scenario['expected_topics'])\n",
                "    has_disclaimer = 'MiFID' in response or 'recomandare' in response.lower()\n",
                "    disclaimer_ok = has_disclaimer == scenario['should_have_disclaimer']\n",
                "    overall = topic_score * 0.7 + (1.0 if disclaimer_ok else 0.0) * 0.3\n",
                "    \n",
                "    agent_results.append({\n",
                "        'Category': scenario['category'],\n",
                "        'Topic Score': f'{topic_score:.0%}',\n",
                "        'Disclaimer OK': '✅' if disclaimer_ok else '❌',\n",
                "        'Overall': f'{overall:.2f}',\n",
                "        'Response Preview': response[:120] + '...',\n",
                "    })\n",
                "    print(f'  Score: {overall:.2f} | Topics: {topic_score:.0%} | Disclaimer: {\"✅\" if disclaimer_ok else \"❌\"}')\n",
                "    print(f'  Response: {response[:120]}...')\n",
                "\n",
                "print('\\n\\n=== Agent Evaluation Summary ===')\n",
                "agent_df = pd.DataFrame(agent_results)\n",
                "display(agent_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print('='*60)\n",
                "print('EVALUATION COMPLETE')\n",
                "print('='*60)\n",
                "print(f'\\nRAG Baseline Scores:  {baseline_scores}')\n",
                "print(f'RAG Reranked Scores:  {reranked_scores}')\n",
                "print(f'Agent Scenarios:      {len(agent_results)} tested')\n",
                "print(f'Agent Pass Rate:      {sum(1 for r in agent_results if float(r[\"Overall\"]) >= 0.7)}/{len(agent_results)}')\n",
                "print('\\nAll results are available in the cells above for the Loom video walkthrough.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}